{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [],
   "source": [
    "from presidio_evaluator.data_generator import PresidioDataGenerator\n",
    "import pandas as pd\n",
    "from presidio_evaluator.data_generator.faker_extensions import (\n",
    "    FakerSpansResult,\n",
    "    RecordsFaker,\n",
    "    NationalityProvider,\n",
    "    OrganizationProvider,\n",
    "    AgeProvider,\n",
    "    AddressProviderNew,\n",
    "    PhoneNumberProviderNew,\n",
    ")\n",
    "import pprint\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from presidio_evaluator import InputSample\n",
    "from typing import Dict, List\n",
    "import tqdm\n",
    "from presidio_evaluator.validation import split_dataset, save_to_json\n",
    "from datetime import date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates_file_path = './data/train_templates.txt'\n",
    "sentence_templates = PresidioDataGenerator.read_template_file(templates_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_name_generator_file = 'data/FakeNameGenerator.com_3000.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_name_generator_df = pd.read_csv(fake_name_generator_file)\n",
    "fake_name_generator_df = fake_name_generator_df[fake_name_generator_df[\"NameSet\"].isin(nationalities)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = PresidioDataGenerator()\n",
    "fake_name_generator_df = PresidioDataGenerator.update_fake_name_generator_df(fake_name_generator_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = RecordsFaker(records=fake_name_generator_df)\n",
    "fake.add_provider(OrganizationProvider)\n",
    "fake.add_provider(AddressProviderNew)\n",
    "fake.add_provider(PhoneNumberProviderNew)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = PresidioDataGenerator(\n",
    "    custom_faker=fake, lower_case_ratio=0.5\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add aliases for classes not available in the Faker library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator.add_provider_alias(\n",
    "    provider_name=\"credit_card_number\", new_name=\"credit_card\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_records = data_generator.generate_fake_data(\n",
    "    templates=sentence_templates, n_samples=180\n",
    ")\n",
    "\n",
    "fake_records = list(fake_records)\n",
    "pprint.pprint(fake_records[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Data Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_per_template_id = Counter([sample.template_id for sample in fake_records])\n",
    "\n",
    "print(f\"Total: {sum(count_per_template_id.values())}\")\n",
    "print(f\"Avg # of records per template: {np.mean(list(count_per_template_id.values()))}\")\n",
    "print(f\"Median # of records per template: {np.median(list(count_per_template_id.values()))}\")\n",
    "print(f\"Std: {np.std(list(count_per_template_id.values()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_per_entity = Counter()\n",
    "for record in fake_records:\n",
    "    count_per_entity.update(Counter([span.type for span in record.spans]))\n",
    "\n",
    "count_per_entity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate entity types (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = {\n",
    "    \"iban\": \"IBAN_CODE\",\n",
    "    \"company\": \"ORGANIZATION\",\n",
    "    \"organization\": \"ORGANIZATION\",\n",
    "    \"name_female\": \"PERSON\",\n",
    "    \"address\": \"STREET_ADDRESS\",\n",
    "    \"country\": \"GPE\",\n",
    "    \"state\": \"GPE\",\n",
    "    \"credit_card\": \"CREDIT_CARD\",\n",
    "    \"city\": \"GPE\",\n",
    "    \"street_name\": \"STREET_ADDRESS\",\n",
    "    \"building_number\": \"STREET_ADDRESS\",\n",
    "    \"name\": \"PERSON\",\n",
    "    \"last_name\": \"PERSON\",\n",
    "    \"last_name_male\": \"PERSON\",\n",
    "    \"last_name_female\": \"PERSON\",\n",
    "    \"first_name\": \"PERSON\",\n",
    "    \"first_name_male\": \"PERSON\",\n",
    "    \"first_name_female\": \"PERSON\",\n",
    "    \"phone_number\": \"PHONE_NUMBER\",\n",
    "    \"email\": \"EMAIL_ADDRESS\",\n",
    "    \"date_time\": \"DATE_TIME\",\n",
    "    \"date_of_birth\": \"DATE_TIME\",\n",
    "    \"day_of_week\": \"DATE_TIME\",\n",
    "    \"name_male\": \"PERSON\",\n",
    "    \"prefix_male\": \"TITLE\",\n",
    "    \"prefix_female\": \"TITLE\",\n",
    "    \"prefix\": \"TITLE\",\n",
    "    \"nationality\": \"NRP\",\n",
    "    \"first_name_nonbinary\": \"PERSON\",\n",
    "    \"postcode\": \"STREET_ADDRESS\",\n",
    "    \"secondary_address\": \"STREET_ADDRESS\",\n",
    "    \"job\": \"TITLE\",\n",
    "    \"state_abbr\": \"GPE\",\n",
    "    \"age\": \"AGE\",\n",
    "}\n",
    "\n",
    "def update_entity_types(dataset:List[FakerSpansResult], entity_mapping:Dict[str,str]):\n",
    "    \"\"\"Replace entity types using a translator dictionary.\"\"\"\n",
    "\n",
    "    for sample in dataset:\n",
    "        # update entity types on spans\n",
    "        for span in sample.spans:\n",
    "            span.type = entity_mapping[span.type]\n",
    "        # update entity types on the template string\n",
    "        for key, value in entity_mapping.items():\n",
    "            sample.template = sample.template.replace(\"{{\" + key + \"}}\", \"{{\" + value + \"}}\")\n",
    "\n",
    "update_entity_types(fake_records, entity_mapping=translator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the fake records containing text utterances, spans and templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:01<00:00, 168.29it/s]\n"
     ]
    }
   ],
   "source": [
    "input_samples = [\n",
    "    InputSample.from_faker_spans_result(faker_spans_result=fake_record, scheme=\"BIO\")\n",
    "    for fake_record in tqdm.tqdm(fake_records)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TEST_RATIOS = [0.7,0.3]\n",
    "train,test = split_dataset(input_samples, TRAIN_TEST_RATIOS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE_DATE = date.today().strftime(\"%b-%d-%Y\")\n",
    "\n",
    "save_to_json(train, \"./data/train_{}.json\".format(DATE_DATE))\n",
    "save_to_json(test, \"./data/val_{}.json\".format(DATE_DATE))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_json(\"./data/train_Apr-03-2023.json\")\n",
    "val_data = pd.read_json(\"./data/val_Mar-22-2023.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_from_span(data, field=\"entity_type\"):\n",
    "    \"\"\"\n",
    "    Count number of entities of each type in a given dataset.\n",
    "    Args:\n",
    "        data: Dataset contining entities of different types generated from the templates (DataFrame).\n",
    "    \"\"\"\n",
    "    spans = data.spans\n",
    "    entities = []\n",
    "    for span in spans:\n",
    "        if len(span) > 0:\n",
    "            for s in span:\n",
    "                entities.append(s[field])\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_entity_types(data: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Count number of entities of each type in a given dataset.\n",
    "    Args:\n",
    "        data: Dataset contining entities of different types generated from the templates (DataFrame).\n",
    "    \"\"\"\n",
    "    count_per_entity = Counter()\n",
    "    for record in data:\n",
    "        for span in record.spans:\n",
    "            count_per_entity[span.type] += 1\n",
    "\n",
    "    print(count_per_entity.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_entity_count = Counter(entity_from_span(train_data, \"entity_value\"))\n",
    "val_entity_count = Counter(entity_from_span(val_data, \"entity_value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_unique_entity_count(data: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Test to make sure all entities in the dataset are unique.\n",
    "    Args:\n",
    "        data: Dataset containing entities (DataFrame) \n",
    "    \"\"\"\n",
    "    entity_count = Counter(entity_from_span(data, \"entity_value\"))\n",
    "    entities = list(entity_count.keys())\n",
    "    duplicate_entities = [entity for entity in entity_count.keys() if entity_count[entity] > 1]\n",
    "    print(duplicate_entities)\n",
    "    # print(len(entities))\n",
    "    assert sum(entity_count.values()) == len(entities)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_no_duplicate_entity(train: pd.DataFrame, val: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Test to ensure there are no entries repeated across two datasets.\n",
    "    Args:\n",
    "        train: Training dataset with all the utterances generated by the templates (DataFrame)\n",
    "        val: Validation dataset with all the utterances generated by the templates (DataFrame)\n",
    "    \"\"\"\n",
    "    train_entities = entity_from_span(train, \"entity_value\")\n",
    "    val_entities = entity_from_span(val, \"entity_value\")\n",
    "    intersection = set(train_entities) & set(val_entities)\n",
    "    assert len(intersection) == 0, print(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_missing_classes(train: pd.DataFrame, val: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Test to ensure there are no classes missing in the validation dataset.\n",
    "    Args:\n",
    "        train: Training dataset with all the utterances generated by the templates (DataFrame)\n",
    "        val: Validation dataset with all the utterances generated by the templates (DataFrame)\n",
    "    \"\"\"\n",
    "    train_entities = entity_from_span(train, \"entity_type\")\n",
    "    val_entities = entity_from_span(val, \"entity_type\")\n",
    "    print(set(val_entities))\n",
    "    intersection = set(train_entities) & set(val_entities)\n",
    "    print(intersection)\n",
    "    print(set(train_entities))\n",
    "    assert len(intersection) == len(set(train_entities)), f\"{intersection}\"\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_unique_entity_count(train_data)\n",
    "test_unique_entity_count(val_data)\n",
    "test_no_duplicate_entity(train_data, val_data)\n",
    "test_missing_classes(train_data, val_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slot_filling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eccf0fd60d2f702fb925cf0205c9b5fbc1c6644f89748d2ccbe2603bf5ec9081"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
